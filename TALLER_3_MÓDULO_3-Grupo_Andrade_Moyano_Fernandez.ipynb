{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de TALLER 3 MÓDULO 3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arvaloplz/Arvaloplz.github.io/blob/master/TALLER_3_M%C3%93DULO_3-Grupo_Andrade_Moyano_Fernandez.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aprendamos a usar Keras\n",
        "Keras es una librería para python que nos permite crear redes neuronales de manera rápida y eficiente. De esta manera nos evitamos las implementaciones de componentes más complejos y nos podemos concentrar en encontrar los parámetros apropiados para nuestra tarea de predicción. \n",
        "\n",
        "Esta vez haremos una tarea de clasificación, específicamente del [dataset de flores Iris para clasificación de flores](https://es.wikipedia.org/wiki/Conjunto_de_datos_flor_iris) a partir de las medidas del sepalo y sus pétalos.\n",
        "\n",
        "<p align=center>\n",
        "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/1920px-Iris_virginica.jpg\" height=\"200\">\n",
        "</p>\n",
        "\n",
        "Primero se deben importar las librerías necesarias. Utilizaremos Keras para la creación de la red neuronal, Pandas para leer nuestro csv con los datos de enrtenamiento y algunas funciones de sklearn para procesar estos datos."
      ],
      "metadata": {
        "id": "c7n2qxcXXQ6y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "qrIKmAFNXNb-"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler # ? \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con pd.read_csv() podemos leer un csv a partir de un archivo local o una url."
      ],
      "metadata": {
        "id": "njW_c4gvPf_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = datasets.load_breast_cancer()\n",
        "df = pd.DataFrame( data.data , columns=data.feature_names)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "dzp26mEBPg5v",
        "outputId": "482d463f-5041-4086-93e0-4d2446b1cf62"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
              "0        17.99         10.38          122.80     1001.0          0.11840   \n",
              "1        20.57         17.77          132.90     1326.0          0.08474   \n",
              "2        19.69         21.25          130.00     1203.0          0.10960   \n",
              "3        11.42         20.38           77.58      386.1          0.14250   \n",
              "4        20.29         14.34          135.10     1297.0          0.10030   \n",
              "\n",
              "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
              "0           0.27760          0.3001              0.14710         0.2419   \n",
              "1           0.07864          0.0869              0.07017         0.1812   \n",
              "2           0.15990          0.1974              0.12790         0.2069   \n",
              "3           0.28390          0.2414              0.10520         0.2597   \n",
              "4           0.13280          0.1980              0.10430         0.1809   \n",
              "\n",
              "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
              "0                 0.07871  ...         25.38          17.33           184.60   \n",
              "1                 0.05667  ...         24.99          23.41           158.80   \n",
              "2                 0.05999  ...         23.57          25.53           152.50   \n",
              "3                 0.09744  ...         14.91          26.50            98.87   \n",
              "4                 0.05883  ...         22.54          16.67           152.20   \n",
              "\n",
              "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
              "0      2019.0            0.1622             0.6656           0.7119   \n",
              "1      1956.0            0.1238             0.1866           0.2416   \n",
              "2      1709.0            0.1444             0.4245           0.4504   \n",
              "3       567.7            0.2098             0.8663           0.6869   \n",
              "4      1575.0            0.1374             0.2050           0.4000   \n",
              "\n",
              "   worst concave points  worst symmetry  worst fractal dimension  \n",
              "0                0.2654          0.4601                  0.11890  \n",
              "1                0.1860          0.2750                  0.08902  \n",
              "2                0.2430          0.3613                  0.08758  \n",
              "3                0.2575          0.6638                  0.17300  \n",
              "4                0.1625          0.2364                  0.07678  \n",
              "\n",
              "[5 rows x 30 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e75880c4-acc6-4bf4-86d7-171bdbeacd1a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>...</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>...</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>...</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>...</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>...</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>...</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 30 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e75880c4-acc6-4bf4-86d7-171bdbeacd1a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e75880c4-acc6-4bf4-86d7-171bdbeacd1a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e75880c4-acc6-4bf4-86d7-171bdbeacd1a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos ver, nuestro dataset contiene 4 características respectivas a las flores y 1 clase (como string) del tipo de flor al que pertenece. Debemos separar esto para poder hacer uso de nuestros algoritmos.\n",
        "\n",
        "Además, es importante para el correcto rendimiento de la red, que cada característica esté normalizada y que las clases estén en formato numérico. "
      ],
      "metadata": {
        "id": "XLmGGFbMPpou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['target'] = data.target\n",
        "Y = pd.get_dummies(df['target']) #   separo etiquetas\n",
        "X = df.drop(columns=['target']) #    de los datos \n",
        "X = (X-X.mean())/X.std() #            y normalizo\n"
      ],
      "metadata": {
        "id": "SBCl7ozzCRn5"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, hay que dividir los datos en Train y Test. Durante el entrenamiento, la red sólo verá los primeros datos, esto es para asegurarse que no esté memorizando solamente. Usualmente la proporción para dividir estos datos es 70/30, siendo mayor la cantidad de datos para entrenamiento."
      ],
      "metadata": {
        "id": "S6tvv6RbSgEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data set into training and testing\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X, Y, test_size=0.3, random_state=2)\n"
      ],
      "metadata": {
        "id": "JtRD8H0i6Cga"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con los datos ya procesados y divididos en set de entrenamiento y test, ya podemos crear la red. Gracias a keras, este proceso es muy simple!"
      ],
      "metadata": {
        "id": "pzS8YFNTQcDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.engine import input_spec\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "modelo = Sequential()\n",
        "modelo.add(Dense(10, input_dim=30, activation=\"relu\"))\n",
        "modelo.add(Dense(10, activation=\"relu\"))\n",
        "modelo.add(Dense(10, activation=\"relu\"))\n",
        "modelo.add(Dense(2, activation=\"softmax\"))\n",
        "\n",
        "modelo.compile(loss='mse', optimizer='adam')\n"
      ],
      "metadata": {
        "id": "hzz1EVKX7NxU"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El código anterior definió una red neuronal, con una capa de entrada de 4 neuronas, 3 capas ocultas de 8 neuronas cada una y una capa de salida con 3 neuronas correspondientes a las clases. La última capa realiza una función de activación *softmax*, que permite transformar las salidas de las neuronas en una probabilidad, de modo que la neurona con el valor más alto será la seleccionada.\n",
        "\n",
        "Finalmente, el modelo se optimizará con una función de pérdida del error cuadrático medio y con un optimizador *adam*, por 150 epocas."
      ],
      "metadata": {
        "id": "VDx3tvBCQqoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = modelo.fit(X_train, Y_train, epochs=150, batch_size=10, validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6ihsA_09NzS",
        "outputId": "b833a929-ed9e-4563-82f7-b7727fd10b85"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "36/36 [==============================] - 1s 8ms/step - loss: 0.2254 - val_loss: 0.1811\n",
            "Epoch 2/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.1551 - val_loss: 0.1191\n",
            "Epoch 3/150\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.0956 - val_loss: 0.0694\n",
            "Epoch 4/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0547 - val_loss: 0.0467\n",
            "Epoch 5/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0365 - val_loss: 0.0392\n",
            "Epoch 6/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0283 - val_loss: 0.0354\n",
            "Epoch 7/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0340\n",
            "Epoch 8/150\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.0213 - val_loss: 0.0323\n",
            "Epoch 9/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0193 - val_loss: 0.0320\n",
            "Epoch 10/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0181 - val_loss: 0.0305\n",
            "Epoch 11/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0170 - val_loss: 0.0300\n",
            "Epoch 12/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0160 - val_loss: 0.0287\n",
            "Epoch 13/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0150 - val_loss: 0.0285\n",
            "Epoch 14/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0144 - val_loss: 0.0276\n",
            "Epoch 15/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0136 - val_loss: 0.0260\n",
            "Epoch 16/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0132 - val_loss: 0.0246\n",
            "Epoch 17/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0122 - val_loss: 0.0224\n",
            "Epoch 18/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0116 - val_loss: 0.0216\n",
            "Epoch 19/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0112 - val_loss: 0.0202\n",
            "Epoch 20/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0105 - val_loss: 0.0197\n",
            "Epoch 21/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0101 - val_loss: 0.0185\n",
            "Epoch 22/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0098 - val_loss: 0.0172\n",
            "Epoch 23/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0094 - val_loss: 0.0163\n",
            "Epoch 24/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0091 - val_loss: 0.0160\n",
            "Epoch 25/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0088 - val_loss: 0.0148\n",
            "Epoch 26/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0086 - val_loss: 0.0149\n",
            "Epoch 27/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0084 - val_loss: 0.0144\n",
            "Epoch 28/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0081 - val_loss: 0.0135\n",
            "Epoch 29/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0079 - val_loss: 0.0136\n",
            "Epoch 30/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0078 - val_loss: 0.0130\n",
            "Epoch 31/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0075 - val_loss: 0.0127\n",
            "Epoch 32/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0073 - val_loss: 0.0129\n",
            "Epoch 33/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0071 - val_loss: 0.0121\n",
            "Epoch 34/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0069 - val_loss: 0.0127\n",
            "Epoch 35/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0069 - val_loss: 0.0124\n",
            "Epoch 36/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0067 - val_loss: 0.0127\n",
            "Epoch 37/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0066 - val_loss: 0.0125\n",
            "Epoch 38/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0065 - val_loss: 0.0128\n",
            "Epoch 39/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0064 - val_loss: 0.0125\n",
            "Epoch 40/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0063 - val_loss: 0.0130\n",
            "Epoch 41/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0063 - val_loss: 0.0124\n",
            "Epoch 42/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0062 - val_loss: 0.0123\n",
            "Epoch 43/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0061 - val_loss: 0.0126\n",
            "Epoch 44/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0061 - val_loss: 0.0129\n",
            "Epoch 45/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0060 - val_loss: 0.0127\n",
            "Epoch 46/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0060 - val_loss: 0.0131\n",
            "Epoch 47/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0060 - val_loss: 0.0132\n",
            "Epoch 48/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0059 - val_loss: 0.0129\n",
            "Epoch 49/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0059 - val_loss: 0.0131\n",
            "Epoch 50/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0059 - val_loss: 0.0132\n",
            "Epoch 51/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0058 - val_loss: 0.0132\n",
            "Epoch 52/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0058 - val_loss: 0.0131\n",
            "Epoch 53/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0058 - val_loss: 0.0138\n",
            "Epoch 54/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0058 - val_loss: 0.0135\n",
            "Epoch 55/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0057 - val_loss: 0.0137\n",
            "Epoch 56/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0057 - val_loss: 0.0138\n",
            "Epoch 57/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0057 - val_loss: 0.0138\n",
            "Epoch 58/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0057 - val_loss: 0.0142\n",
            "Epoch 59/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0057 - val_loss: 0.0142\n",
            "Epoch 60/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0057 - val_loss: 0.0143\n",
            "Epoch 61/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0057 - val_loss: 0.0142\n",
            "Epoch 62/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0057 - val_loss: 0.0145\n",
            "Epoch 63/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0057 - val_loss: 0.0145\n",
            "Epoch 64/150\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.0056 - val_loss: 0.0146\n",
            "Epoch 65/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0056 - val_loss: 0.0147\n",
            "Epoch 66/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0056 - val_loss: 0.0151\n",
            "Epoch 67/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0056 - val_loss: 0.0151\n",
            "Epoch 68/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0056 - val_loss: 0.0153\n",
            "Epoch 69/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0056 - val_loss: 0.0156\n",
            "Epoch 70/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0056 - val_loss: 0.0156\n",
            "Epoch 71/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0056 - val_loss: 0.0158\n",
            "Epoch 72/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0056 - val_loss: 0.0162\n",
            "Epoch 73/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0055 - val_loss: 0.0168\n",
            "Epoch 74/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0055 - val_loss: 0.0172\n",
            "Epoch 75/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0055 - val_loss: 0.0177\n",
            "Epoch 76/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0055 - val_loss: 0.0183\n",
            "Epoch 77/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0055 - val_loss: 0.0189\n",
            "Epoch 78/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0055 - val_loss: 0.0198\n",
            "Epoch 79/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0054 - val_loss: 0.0200\n",
            "Epoch 80/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0054 - val_loss: 0.0205\n",
            "Epoch 81/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0054 - val_loss: 0.0206\n",
            "Epoch 82/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0053 - val_loss: 0.0210\n",
            "Epoch 83/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0053 - val_loss: 0.0211\n",
            "Epoch 84/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0053 - val_loss: 0.0205\n",
            "Epoch 85/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0053 - val_loss: 0.0207\n",
            "Epoch 86/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0053 - val_loss: 0.0199\n",
            "Epoch 87/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0052 - val_loss: 0.0199\n",
            "Epoch 88/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0052 - val_loss: 0.0195\n",
            "Epoch 89/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0052 - val_loss: 0.0193\n",
            "Epoch 90/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0052 - val_loss: 0.0193\n",
            "Epoch 91/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0051 - val_loss: 0.0197\n",
            "Epoch 92/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0051 - val_loss: 0.0193\n",
            "Epoch 93/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0051 - val_loss: 0.0205\n",
            "Epoch 94/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0050 - val_loss: 0.0203\n",
            "Epoch 95/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0050 - val_loss: 0.0200\n",
            "Epoch 96/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0050 - val_loss: 0.0200\n",
            "Epoch 97/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0050 - val_loss: 0.0197\n",
            "Epoch 98/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0050 - val_loss: 0.0189\n",
            "Epoch 99/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0201\n",
            "Epoch 100/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0210\n",
            "Epoch 101/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0049 - val_loss: 0.0207\n",
            "Epoch 102/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0219\n",
            "Epoch 103/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0048 - val_loss: 0.0215\n",
            "Epoch 104/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0045 - val_loss: 0.0201\n",
            "Epoch 105/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0044 - val_loss: 0.0222\n",
            "Epoch 106/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0044 - val_loss: 0.0229\n",
            "Epoch 107/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0044 - val_loss: 0.0224\n",
            "Epoch 108/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0042 - val_loss: 0.0251\n",
            "Epoch 109/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0041 - val_loss: 0.0253\n",
            "Epoch 110/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0040 - val_loss: 0.0267\n",
            "Epoch 111/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0038 - val_loss: 0.0280\n",
            "Epoch 112/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0039 - val_loss: 0.0252\n",
            "Epoch 113/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0038 - val_loss: 0.0313\n",
            "Epoch 114/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0038 - val_loss: 0.0327\n",
            "Epoch 115/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0035 - val_loss: 0.0326\n",
            "Epoch 116/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0035 - val_loss: 0.0303\n",
            "Epoch 117/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0026 - val_loss: 0.0288\n",
            "Epoch 118/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0023 - val_loss: 0.0296\n",
            "Epoch 119/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0019 - val_loss: 0.0290\n",
            "Epoch 120/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0017 - val_loss: 0.0318\n",
            "Epoch 121/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0015 - val_loss: 0.0318\n",
            "Epoch 122/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0323\n",
            "Epoch 123/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0341\n",
            "Epoch 124/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 0.0012 - val_loss: 0.0323\n",
            "Epoch 125/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.0344\n",
            "Epoch 126/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 9.0461e-04 - val_loss: 0.0344\n",
            "Epoch 127/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 7.9808e-04 - val_loss: 0.0345\n",
            "Epoch 128/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 6.7616e-04 - val_loss: 0.0346\n",
            "Epoch 129/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 5.9435e-04 - val_loss: 0.0348\n",
            "Epoch 130/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 5.0869e-04 - val_loss: 0.0342\n",
            "Epoch 131/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 4.4867e-04 - val_loss: 0.0341\n",
            "Epoch 132/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3.8644e-04 - val_loss: 0.0335\n",
            "Epoch 133/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 3.6844e-04 - val_loss: 0.0329\n",
            "Epoch 134/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3.2779e-04 - val_loss: 0.0330\n",
            "Epoch 135/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.7257e-04 - val_loss: 0.0323\n",
            "Epoch 136/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2.4398e-04 - val_loss: 0.0320\n",
            "Epoch 137/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2.2065e-04 - val_loss: 0.0318\n",
            "Epoch 138/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 2.0013e-04 - val_loss: 0.0317\n",
            "Epoch 139/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 1.8375e-04 - val_loss: 0.0314\n",
            "Epoch 140/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 1.6767e-04 - val_loss: 0.0312\n",
            "Epoch 141/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1.5068e-04 - val_loss: 0.0310\n",
            "Epoch 142/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1.4274e-04 - val_loss: 0.0311\n",
            "Epoch 143/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 1.2855e-04 - val_loss: 0.0308\n",
            "Epoch 144/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 1.1895e-04 - val_loss: 0.0308\n",
            "Epoch 145/150\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.1248e-04 - val_loss: 0.0307\n",
            "Epoch 146/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 1.0835e-04 - val_loss: 0.0310\n",
            "Epoch 147/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 9.8388e-05 - val_loss: 0.0310\n",
            "Epoch 148/150\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 8.6118e-05 - val_loss: 0.0309\n",
            "Epoch 149/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 8.1137e-05 - val_loss: 0.0308\n",
            "Epoch 150/150\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 7.6736e-05 - val_loss: 0.0308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver como efectivamente, durante el entrenamiento, la función de coste nos indicó que el modelo va mejorando en sus predicciones."
      ],
      "metadata": {
        "id": "jRxzAxmPTN4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss']) # corresponden al test\n",
        "plt.plot(history.history['val_loss'])# corresponden al train\n",
        "plt.title('Función de coste')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "rIt-3bQ_RrdW",
        "outputId": "dc467965-830e-488f-9c96-437987749dbe"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, finalmente podemos realizar métricas de rendimiento del modelo cuando este predice los datos de Test. Para la tarea de clasificación, podemos utilizar la presición, recall o el f1-score. Todas estas métricas se basan en los errores tipo 1 (Falso positivo) y tipo 2(Falso negativo).\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://www.researchgate.net/profile/Sebastian-Bittrich/publication/330174519/figure/fig1/AS:711883078258689@1546737560677/Confusion-matrix-Exemplified-CM-with-the-formulas-of-precision-PR-recall-RE.png\" height=\"220\">\n",
        "</p>"
      ],
      "metadata": {
        "id": "heFu2mV8TWWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, multilabel_confusion_matrix, precision_score\n",
        "Y_pred = modelo.predict(X_test)\n",
        "Y_exp = Y_test.to_numpy()\n",
        "for i in range(len(Y_pred)):\n",
        "  Y_pred[i] = np.array([1 if Y_pred[i][j] == np.max(Y_pred[i]) else 0 for j in range(len(Y_pred[i]))])\n",
        "  \n",
        "\n",
        "print(accuracy_score(Y_exp, Y_pred))\n",
        "print(classification_report(Y_exp, Y_pred))\n",
        "\n",
        "matriz_confusion = multilabel_confusion_matrix(Y_exp, Y_pred)\n",
        "print(matriz_confusion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMPR362TDw1T",
        "outputId": "7207ef7d-a32b-4349-8e52-0b309022cee6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9532163742690059\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.94      0.94        67\n",
            "           1       0.96      0.96      0.96       104\n",
            "\n",
            "   micro avg       0.95      0.95      0.95       171\n",
            "   macro avg       0.95      0.95      0.95       171\n",
            "weighted avg       0.95      0.95      0.95       171\n",
            " samples avg       0.95      0.95      0.95       171\n",
            "\n",
            "[[[100   4]\n",
            "  [  4  63]]\n",
            "\n",
            " [[ 63   4]\n",
            "  [  4 100]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "clases = ['no se detecta', 'se detecta']\n",
        "\n",
        "for i, matrix in enumerate(matriz_confusion):\n",
        "  labels = [f'True Neg\\n{matrix[0][0]}',f'False Pos\\n{matrix[0][1]}',f'False Neg\\n{matrix[1][0]}',f'True Pos\\n{matrix[1][1]}']\n",
        "\n",
        "  labels = np.asarray(labels).reshape(2,2)\n",
        "\n",
        "  ax = sns.heatmap(matriz_confusion[2], annot=labels, fmt='', cmap='rocket_r')\n",
        "\n",
        "  ax.set_title(f'Matriz de Confusión para {clases[i]}\\n\\n');\n",
        "  ax.set_xlabel('\\nPredicted Values')\n",
        "  ax.set_ylabel('Actual Values ');\n",
        "\n",
        "  ## Ticket labels - List must be in alphabetical order\n",
        "  ax.xaxis.set_ticklabels(['False','True'])\n",
        "  ax.yaxis.set_ticklabels(['False','True'])\n",
        "\n",
        "  ## Display the visualization of the Confusion Matrix.\n",
        "  plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "zF0uCWKlMrlC",
        "outputId": "493949ac-fd33-4a4c-8c85-05f3d93dcbbe"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-a382cfeb2724>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatriz_confusion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rocket_r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Matriz de Confusión para {clases[i]}\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora prueben cambiando la arquitectura de la red o el dataset y su procesamiento."
      ],
      "metadata": {
        "id": "wxOgeMJPXSI8"
      }
    }
  ]
}